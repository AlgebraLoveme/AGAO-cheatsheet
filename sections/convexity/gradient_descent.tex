\section{Solving Convex Optimization approximately}

\subsection{Solving via Gradient Descent}

\subsubsection{Vanilla Gradient Descent}
\textbf{Theorem}: Let $f: \mathbb{R}^{n} \rightarrow \mathbb{R}$ be a $\beta$-gradient Lipschitz, convex function. Let $\boldsymbol{x}_{0}$ be a given starting point, and let $\boldsymbol{x}^{*} \in \arg \min _{x \in \mathbb{R}^{n}} f(\boldsymbol{x})$ be a minimizer of $f$. The Gradient Descent algorithm given by
$
\boldsymbol{x}_{i+1}=\boldsymbol{x}_{i}-\frac{1}{\beta} \boldsymbol{\nabla} f\left(\boldsymbol{x}_{i}\right)
$
ensures that the kth iterate satisfies
$
f\left(\boldsymbol{x}_{k}\right)-f\left(\boldsymbol{x}^{*}\right) \leq \frac{2 \beta\left\|\boldsymbol{x}_{0}-\boldsymbol{x}^{*}\right\|_{2}^{2}}{k+1} .
$

\subsubsection{Accelerated Gradient Descent}
\textbf{Theorem}: Let $f: \mathbb{R}^{n} \rightarrow \mathbb{R}$ be a $\beta$-gradient Lipschitz, convex function. 
The Accelerated Gradient Descent algorithm given by
$$
\begin{aligned}
a_{i} &=\frac{i+1}{2}, A_{i}=\frac{(i+1)(i+2)}{4} \\
\boldsymbol{v}_{0} &=\boldsymbol{x}_{0}-\frac{1}{2 \beta} \boldsymbol{\nabla} f\left(\boldsymbol{x}_{0}\right) \\
\boldsymbol{y}_{i} &=\boldsymbol{x}_{i}-\frac{1}{\beta} \boldsymbol{\nabla} f\left(\boldsymbol{x}_{i}\right) \\
\boldsymbol{x}_{i+1} &=\frac{A_{i} \boldsymbol{y}_{i}+a_{i+1} \boldsymbol{v}_{i}}{A_{i+1}} \\
\boldsymbol{v}_{i+1} &=\boldsymbol{v}_{i}-\frac{a_{i+1}}{\beta} \boldsymbol{\nabla} f\left(\boldsymbol{x}_{i+1}\right)
\end{aligned}
$$
ensures that the kth iterate satisfies
$
f\left(\boldsymbol{x}_{k}\right)-f\left(\boldsymbol{x}^{*}\right) \leq \frac{2 \beta\left\|\boldsymbol{x}_{0}-\boldsymbol{x}^{*}\right\|_{2}^{2}}{(k+1)(k+2)} .
$

\subsection{Solving via Newton's Method}
