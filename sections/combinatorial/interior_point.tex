\section{Interior Point Method for Max Flow: Barrier Method}

Undirected max flow: Given a source vertex and a sink vertex, maximize $F$ such that $\boldsymbol{B}\boldsymbol{f} = F (-\boldsymbol{1}_s + \boldsymbol{1}_t)$ and $-\boldsymbol{c} \le \boldsymbol{f} \le \boldsymbol{c}$.

\textbf{Core idea}: convert the inequality constraints to the object function via logrithm, as the logrithm only takes positive inputs. Then, if the object has $K$-stable Hessians, we can quickly solve it using Newton's method. 

For the undirected max flow, we define the barrier function: $V(\boldsymbol{f})=\sum_{e}-\log (\boldsymbol{c}(e)-\boldsymbol{f}(e))-\log (\boldsymbol{c}(e)+\boldsymbol{f}(e)).$ Given a constant $0\le \alpha <1$ and the value for the max flow $F^*$, we define the barrier problem to be minimizing $V(\boldsymbol{f})$ such that $B\boldsymbol{f} = \alpha F^* (-\boldsymbol{1}_s + \boldsymbol{1}_t)$. The KKT condition for the barrier problem is: (1) primal feasibility, \ie, $Bf = \alpha F^* (-\boldsymbol{1}_s + \boldsymbol{1}_t)$ and $-\boldsymbol{c} \le \boldsymbol{f} \le \boldsymbol{c}$, (2) gradient condition, \ie, $\nabla V(\boldsymbol{f}) = B^\top \boldsymbol{x}$ for the multiplier $\boldsymbol{x}$.

Let $\boldsymbol{f}_\alpha$ and $\boldsymbol{x}_\alpha$ be the optima of the barrier problem. The interior point method is based on the fact that given $\boldsymbol{f}_\alpha$ and $\boldsymbol{x}_\alpha$, we can quickly compute the optimal flow at $\alpha + \alpha^\prime$ for some $\alpha^\prime < 1-\alpha$ using Newton's method. By repeatly doing this, we can get the optima for $\alpha > 1-\epsilon$, thus a good approximation of the max flow. If the approximation is good enough for interger-weighted graphs, then we get the exact solution. Note that we can binary search for $F^*$ in log time.

\subsection{Updates using Divergence}

The update problem is to minimize $V(\boldsymbol{f} + \boldsymbol{\delta})$ such that $B\boldsymbol{\delta} = \alpha^\prime F^* (-\boldsymbol{1}_s + \boldsymbol{1}_t)$. Actually, as $V\left(\boldsymbol{\delta}+\boldsymbol{f}_{\alpha}^{*}\right)-\left(V\left(\boldsymbol{f}_{\alpha}^{*}\right)+\left\langle\boldsymbol{\nabla} V\left(\boldsymbol{f}_{\alpha}^{*}\right), \boldsymbol{\delta}\right\rangle\right)=V\left(\boldsymbol{\delta}+\boldsymbol{f}_{\alpha}^{*}\right)-\left(V\left(\boldsymbol{f}_{\alpha}^{*}\right)+\left\langle\boldsymbol{x}_{\alpha}^{*}, \alpha^{\prime} F^{*} \boldsymbol{b}_{s t}\right\rangle\right)$ by the KKT gradient condition, the \textbf{divergence update problem} to minimize $V(\boldsymbol{\delta}+\boldsymbol{f})-(V(\boldsymbol{f})+\langle\boldsymbol{\nabla} V(\boldsymbol{f}), \boldsymbol{\delta}\rangle)$ such that $\boldsymbol{B} \boldsymbol{\delta}=\alpha^{\prime} F^{*} \boldsymbol{b}_{s t}$ has the same optima as the original update problem. The divergence update problem is easier to analyze as we remove the first order information.

\subsection{Fast Divergence Updates via Smoothing}

\textbf{Theorem}: Suppose $S \subseteq \mathbb{R}^{n}$ is a convex set, and let $f, g: S \rightarrow \mathbb{R}$ be convex functions. Let $\boldsymbol{x}^{*}=\arg \min _{\boldsymbol{x} \in S} f(\boldsymbol{x})$. Suppose $f, g$ agree on a neighborhood of $\boldsymbol{x}^{*}$ in $S$ (i.e. an open set containing $\left.\boldsymbol{x}^{*}\right)$. Then $\boldsymbol{x}^{*}=\arg \min _{\boldsymbol{x} \in S} g(\boldsymbol{x})$. The proof is straightforward as local optimum is global for convex functions.

Smoothing is to find another (smoothed) convex function which agrees with the original convex function in the neighborhood of the optima, but with $K$-stable Hessian. Note that our divergence function obtains minimum at zero (here we do not care about constraints). Therefore, we can choose the smoothed function to be the second Taylor expansion at some point for large values, and use the divergence of the log function $-\log(1-x)-x$ for the neighborhood of zero. Then Newton's method (with linear constraints) can solve this in $\tilde{O}(m)$ time.